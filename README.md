# Multimodal Architecture using PyTorch

## Introduction
This project aims to build a multimodal architecture using PyTorch to handle and integrate multiple modalities of data for improved performance in a meme classification task. The architecture will be based on feed forward or multilayer perceptron (MLP) networks and will be trained and evaluated on a relevant dataset to demonstrate its effectiveness.

## Dependencies
- PyTorch
- Python >=3.6
- Jupyter Notebook (optional)
- Numpy
- Matplotlib (optional)
- Sklearn (optional)

## Dataset
The dataset used for training and evaluating the multimodal architecture will be a meme classification dataset containing images and textual captions. The dataset should have multiple examples of memes with their corresponding image and caption for each class.

## Usage
1. Clone the repository `git clone https://github.com/Anas1108/Multimodal_Memes_Classification`
2. Install the dependencies using `pip install -r requirements.txt`
3. Open the Jupyter Notebook and run the cells to train the model.
4. Evaluate the model on the test data to see its performance on the meme classification task.

## Results
The performance of the multimodal architecture will be evaluated using relevant metrics such as accuracy and compared to baselines to demonstrate its effectiveness in handling and integrating multiple modalities of data for improved performance in the meme classification task.

## Conclusion
This project provides a basic implementation of a multimodal architecture using PyTorch for a meme classification task and serves as a starting point for further exploration and improvement.
